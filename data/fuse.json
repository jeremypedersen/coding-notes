{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Coding Notes","n":0.707},"1":{"v":"\nWelcome! I keep all my coding notes here. Anything I learn about writing code, testing code, updating code, or optimizing code will go here.\n\nCheck back from time to time for updates. \n","n":0.177}}},{"i":2,"$":{"0":{"v":"Web Programming","n":0.707},"1":{"v":"\nNotes on web programming.\n\n","n":0.5}}},{"i":3,"$":{"0":{"v":"Web Programming Basics","n":0.577},"1":{"v":"\n# HTML, CSS, JavaScript\n\nThere are three core components that are used to build web applications:\n\n- HTML (Hypertext Markup Language)\n- CSS (Cascading Stylesheets)\n- JavaScript (in modern times, TypeScript)\n\nHTML defines the structure of a web page, by allowing you to \"mark up\" (tag) text, and create links to other pages or media images, videos. HTML is all about structure: it doesn't provide the ability to *style* content. \n\nCSS is used to apply styles to HTML. Whereas HTML definese the *content* and *structure* of a web page, CSS defines the *style*. Colors, sizes, fonts and the like are controlled by CSS.\n\nJavaScript provides dynamic functionality. If something on a webpage moves, changes style or position, or otherwise responds to user behavior, that's usually JavaScript at work.\n\n## Web Standards (how we got HTML5)\n\n- Pre-1997 there were essentially no standards: browsers could implement HTML any way they wanted\n- The first widely adopted standards were HTML 4.0 and 4.01, but this was a loose standard.\n- In 2000, W3C (the standards body responsible for HTML 4) came out with XHTML 1.0, a far more rigid and clear standard.\n- The major browser vendors thought work on XHTML 1.0 was moving too slow, so in 2004 they created WHATWG (Web Hypertext Application Technology Group) to work on their own standard. This is the group that drives the current HTML5 standard.\n- Between ~2007-2009 W3C and WHATWG started working together on HTML5 with WHATWG. \n- Essentially, WHATWG owns the \"living\" implementation of HTML5, and W3C owns the standard \"fixed\" specification of that standard. More [here](https://www.w3.org/blog/2019/w3c-and-whatwg-to-work-together-to-advance-the-open-web-platform/).\n\n### Helpful tools for keeping track of the state of web standards\n\n- [The W3C standards page](https://www.w3.org/standards/)\n- [Can I Use](https://caniuse.com/)\n\n## Anatomy of HTML tags\n\n","n":0.06}}},{"i":4,"$":{"0":{"v":"SQL Notes","n":0.707},"1":{"v":"\nNotes on various SQL dialects. \n","n":0.408}}},{"i":5,"$":{"0":{"v":"MySQL Usage","n":0.707},"1":{"v":"\nNotes on MySQL's flavor of the SQL language.\n\n\n## Section 1: Basic Basics (Create and delete databases and tables, insert records)\n\n**Show what databases are available to work with**\n\n```\nSHOW DATABASES;\n```\n\n**Create a database**\n\n```\nCREATE DATABASE big_company;\n```\n\n**Show which database is in use**\n\n```\nSELECT database();\n```\n\n**Select a database**\n\n```\nUSE big_company;\n```\n\n**Show tables in the database**\n\n```\nSHOW TABLES;\n```\n\n**Create a new table**\n\n```\nCREATE TABLE employees (\n    employee_id INT,\n    name VARCHAR(80),\n    role VARCHAR(80)\n);\n```\n\n**Describe a table (list its fields and datatypes)**\n\n```\nDESC employees;\n```\n\n**Show the contents of a table**\n\n```\nSELECT * FROM employees;\n```\n\n**Delete a table**\n\n```\nDROP TABLE employees;\n```\n\n**Insert a new record into a table**\n\n```\nINSERT INTO employees (employee_id, name, role) \nVALUES (0, 'Jeremy', 'Founder');\n```\n\n**(*Shortcut*) insert a new record into a table, where all fields are specified**\n\n```\nINSERT INTO employees VALUES (0, 'Jeremy', 'Founder');\n```\n\n**Insert mutiple new records into a table**\n\n```\nINSERT INTO employees (employee_id, name, role) \nVALUES \n    (1, 'Erica', 'Co-Founder'),\n    (2, 'James', 'CFO'),\n    (3, 'Leslie', 'CTO');\n```\n\n**Create a table with *required* fields (meaning fields which cannot be `NULL`**\n\n```\nCREATE TABLE students (\n    name VARCHAR(80) NOT NULL,\n    gpa INT NOT NULL\n);\n```\n\n**Create a table with *required* fields and *default* values for those fields**\n\n```\nCREATE TABLE students (\n    name VARCHAR(80) NOT NULL DEFAULT 'noname',\n    gpa INT NOT NULL DEFAULT 0\n);\n```\n\n**Insert an empty record (defaults will get filled in)**\n\n```\nINSERT INTO students () VALUES ();\n```\n\n**Insert a record where only some fields are specified**\n\n```\nINSERT INTO students (name) VALUES ('James');\n```\n\n**Create a table with a primary key**\n\n```\nCREATE TABLE employees (\n    employee_id INT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    team_id INT NOT NULL\n);\n```\n\n**Create a table with a primary key that increments on its own**\n\n```\nCREATE TABLE employees (\n    employee_id INT PRIMARY KEY AUTO_INCREMENT,\n    name VARCHAR(100) NOT NULL,\n    team_id INT NOT NULL\n);\n```\n\n**Insert records into a table with a primary key (identical-looking records can still be differentiated using the primary key)**\n\n```\nINSERT INTO employees (name, team_id) \nVALUES \n    ('Bob', 25),\n    ('Bob', 25),\n    ('Bob', 25);\n```\n\n**(*Optional*) another way to create a primary key which auto-increments**\n\n```\nCREATE TABLE employees (\n    employee_id INT AUTO_INCREMENT,\n    name VARCHAR(100) NOT NULL,\n    team_id INT NOT NULL,\n    PRIMARY KEY(employee_id)\n);\n```\n\n## Section 2: Working with data (CRUD operations)\n\n**CRUD = Create, Read, Update, Delete**\n\n### Create\n\n**Create a database**\n\n```\nCREATE DATABASE big_company;\n```\n\n**Create a table**\n\n```\nCREATE TABLE employees (\n    employee_id INT AUTO_INCREMENT,\n    name VARCHAR(100) NOT NULL,\n    team_id INT NOT NULL,\n    PRIMARY KEY(employee_id)\n);\n```\n\n**Create a record in a table**\n\n```\nINSERT INTO employees (name, team_id) VALUES ('Bob', 25);\n```\n\n### Read\n\n**Select all items from a table (*read*)**\n\n```\nSELECT * FROM employees;\n```\n\n**Select specific columns**\n\n```\nSELECT name FROM employees;\n```\n\n**Select records that meet a certain condition**\n\n```\nSELECT * FROM employees WHERE name = 'Bob';\n```\n\n*Note:* Just because we are using `WHERE` with a specific column, doesn't mean we have to `SELECT` that column. For instance, if I wanted to know which teams had an employee named \"Bob\", I could do this:\n\n```\nSELECT team_id FROM employees WHERE name = 'Bob' ;\n```\n*Note:* By default, string comparisons are *case insensitive* in MySQL. \n\n**Select records where one column's value matches the value of another column**\n\n```\nSELECT * FROM employees where team_id = employee_id;\n```\n\n**Rename one or more columns using `AS`**\n\n```\nSELECT employee_id AS id, name FROM employees; \n```\n\n### Update\n\n**Update an existing row in a table**\n\n```\nUPDATE employees SET name = 'Other Bob' WHERE employee_id = 1;\n```\n\n**Update a single column for all rows at once**\n\n```\nUPDATE employees SET name = 'Whatever';\n```\n\n**Update multiple columns for all rows at once**\n\n```\nUPDATE employees SET name = 'Whatever', team_id = 0;\n```\n\n**Delete one row**\n\n```\nDELETE FROM employees WHERE employee_id = 1;\n```\n\n**Delete one or more rows that match a pattern**\n\n```\nDELETE FROM employees WHERE name = 'Whatever';\n```\n\n**Delete *all* rows**\n\n```\nDELETE FROM employees;\n```\n\n## Section 3: Working with string functions\n\n**Concatenate arbitrary strings**\n\n```\nSELECT CONCAT('h', 'e', 'l', 'l', 'o');\n```\n\n**Concatenate string data from from two columns in a table**\n\n```\nSELECT CONCAT (first_name, ' ', last_name) FROM friends;\n```\n\n**Concatenate and rename the output**\n\n```\nSELECT CONCAT (first_name, ' ', last_name) AS full_name FROM friends ;\n```\n\n**Concatenate with a separator between each item**\n\n```\nSELECT CONCAT_WS (' ', first_name, last_name) AS full_name FROM friends ;\n```\n\n### The `SUBSTRING` Function\n\n`SUBSTRING` is a cool little function that lets you select a portion of a string. \n\n**Select 4 characters, starting from index 1 (the first character)**\n\n```\nSELECT SUBSTRING('Hey there friends', 1, 4);\n```\n\n**Note:** The command above will print out `Hey `, which appears to be 3 characters but is actually four (remember there's a space after the `y`!)\n\n**Select all characters from position 5 to the end of the string**\n\n```\nSELECT SUBSTRING('Hey there friends', 5);\n```\n\n**Note:** This will print `there friends`\n\n**Select the *last* three characters in a string**\n\n```\nSELECT SUBSTRING('Hey there friends', -3);\n```\n\n**Note:** This will print `nds`\n\n### Combining `SUBSTRING` with `CONCAT`\n\nComing soon!","n":0.038}}},{"i":6,"$":{"0":{"v":"Digital Music","n":0.707},"1":{"v":"\nA space where I catalogue tools and experiments in Digital Music.\n\n## Music with Code\n\n- [SonicPi](https://sonic-pi.net/) a Ruby-based music synthesizer which you program by writing actual code (loops and arrays and all that)\n\n## AI / ML Stuff\n\n- [MusicGen](https://huggingface.co/spaces/sanchit-gandhi/musicgen-streaming) a text-to-music model from Facebook research (now Meta)\n\n","n":0.149}}},{"i":7,"$":{"0":{"v":"Computer Vision","n":0.707},"1":{"v":"\nResources related to computer vision. For me this means everything from image processing to stitching panoramas to photogrammetry and even creating and editing NeRF models. \n\nFor now it's just sort of a brain dump where I collect tools I want to play with. Here are a few I'm researching now.\n\nRight now the focus is mostly on traditional CV. Things that are more obviously AI driven like text-to-image models will go into the `ai.` notes tree. \n\n## Focus stacking\n\n- [focus-stack](https://github.com/PetteriAimonen/focus-stack), plus [this blog post](https://peterfalkingham.com/2024/04/29/an-excellent-free-and-open-source-focus-stacking-solution/) talking about how to use it. \n\n## 3D Reconstruction and Photogrammetry\n\n- [micmac](https://github.com/micmacIGN/micmac), open source photogrammetry.\n- [meshroom](https://github.com/alicevision/Meshroom?tab=readme-ov-file), open source 3D reconstruction software\n- A quick [survival guide](https://moviola.com/courses/meshroom-survival-guide/) to using meshroom\n- [nerfstudio](https://docs.nerf.studio/) for building NeRFs.\n- [CloudCompare](https://www.danielgm.net/cc/) open source software for working with 3D point clouds and meshes.\n- [MeshLab](https://www.meshlab.net/) open source system for processing and editing 3D triangular meshes.\n- [Open3D](https://www.open3d.org/), open source software for 3D data processing, with [documenatation here](https://www.open3d.org/docs/release/).\n- [Regard3D](https://www.regard3d.org/index.php/documentation/tutorial) open source software to go from a set of images to a 3D model. \n- [openMVG](http://imagine.enpc.fr/~moulonp/openMVG/index.html) tool for \"multiple view geometry\" targeted to developers. Looks like it makes it easy to solve for geometry using 2 to \"N\" different views of a scene. \n- [openMVS](https://github.com/cdcseacave/openMVS) stereo view reconstruction software. Looks like it can do a ton of cool stuff but I'm mostly interested in the ability to do mesh reconstruction (to make 3D models).\n- [PMVS (Patch-based Multi-vew Stereo)](https://www.di.ens.fr/pmvs/), a tool for doing the 3D reconstruction from a set of images. Apparently it is included inside [CVMS (Clustering Views for Multi-vew Stereo)](https://www.di.ens.fr/cmvs/).\n\nWhile looking for open source photogrammetry solutions, I ran across [this cool blog post from an archaeology team](https://arc-team-open-research.blogspot.com/2016/12/comparing-7-photogrammetry-systems.html) talking about reconstructions done for scientific purposes. \n\nWildly enough, the quality of these models can be quite good. Good enough to [print replacement teeth for dogs](https://www.dailymail.co.uk/sciencetech/article-3678651/Animal-avengers-rescue-Adorable-puppy-eat-3D-printed-tooth-replaces-one-broke-chewing.html), at least.\n\nThey looked at a couple different solutions and chose openMVS + openMVG as their general favorite.\n\nIt looks like for simple 3D reconstructions from a set of photographs Meshlab may also work (not tested in their blog).\n\nAs for **sharing the resulting models** it seems [Thingiverse](https://www.thingiverse.com/) is the go-to for 3D printing, and [Sketchfab](https://sketchfab.com) for general 3D models.  \n\nApparently [Three.js](https://threejs.org/examples/#webgl_animation_keyframes) can also display reconstructions, up to and including animation! \n\n## Panoramas\n\n- [Hugin](https://hugin.sourceforge.io/), which appears to be the leading open source toolkit for making panoramic images, and is based on [Panorama tools](https://panotools.sourceforge.net/). Both seem to be working just fine but are quite old and seem to mostly just be receiving maintenance updates. The releases on the Hugin site indicate that no major new features are being added, but the software is being actively maintained.\n- [Xpano](https://krupkat.github.io/xpano/) looks like a simpler alternative to Hugin, will autodetect related images in a folder, stitch them, and export.\n\n## AI/ML Stuff\n\nThere are some really, really cool demos that mix photography and generative AI. Most of them are hosted on HuggingFace. Here are a couple I've looked at: \n\n- [InstantMesh](https://huggingface.co/spaces/TencentARC/InstantMesh) takes a single photo, generates best-guesses for multiple views of the object, and then uses those to create an actual colorized 3D model in OBJ or GLB format.\n- [StableDesign](https://huggingface.co/spaces/MykolaL/StableDesign) submit an image of a bare interior and a description of the interior design aesthetic you are interested in, and the system generates a concept image showing the space redesigned to meet your needs.\n- [Minecraft skin generator](https://huggingface.co/spaces/Nick088/Stable_Diffusion_Finetuned_Minecraft_Skin_Generator) Use a prompt to generate 3D models which can be used as \"skins\" for player characters in Minecraft.\n- [multimodalart](https://huggingface.co/multimodalart), a really interesting collection of spaces for generating AI art, including spaces to help you [easily create and train LoRAs](https://huggingface.co/spaces/multimodalart/lora-ease) and space to [re-style selfies](https://huggingface.co/spaces/multimodalart/face-to-all).\n\n","n":0.041}}},{"i":8,"$":{"0":{"v":"AI Notes","n":0.707},"1":{"v":"\nNotes on Artificial Intelligence (AI) and Machine Learning (ML), with an emphasis on testing, training, fine-tuning, deploying, and optimizing Machine Learning models.\n\nNotes will be divided into several categories (see links to children below).\n\n","n":0.174}}},{"i":9,"$":{"0":{"v":"Prompting","n":1},"1":{"v":"\nGuides on advanced techniques for writing prompts for AI models, including:\n\n- Prompts for LLMs\n- Prompts for text-to-music/sound\n- Prompts for text-to-image\n- Prompts for text-to-video\n\n# A quick list of the best public prompting resources\n\n- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n- [Anthropic Prompt Library](https://docs.anthropic.com/claude/prompt-library)\n- [LangSmith, by LangChain](https://smith.langchain.com/hub?ref=blog.langchain.dev)\n\n","n":0.156}}},{"i":10,"$":{"0":{"v":"LLMs","n":1},"1":{"v":"\nPrompt engineering for Large Language Models (LLMs) such as [GPT-4](https://openai.com/research/gpt-4) and [Claude 3](https://www.anthropic.com/news/claude-3-family)\n\nSee the children listed below: as times goes on I will try to add model-specific prompting guides here.\n","n":0.183}}},{"i":11,"$":{"0":{"v":"Prompt Engineering: Generic Notes","n":0.5},"1":{"v":"\n**Before you read further**: There is already a very comprehensive prompting guide available [here](https://www.promptingguide.ai/) from Elvis Saravia on GitHub. If you want an exhaustive intro with more details, check that out! I make frequent references to that guide below. **My notes here borrow extensivley from this guide**, because it's great!\n\n# What is a prompt?\n\n**A prompt is the *input* to a Large Language Model (LLM)**. The prompt guides the LLM's output (often called a *response* or *completion*).\n\n```mermaid\nflowchart LR;\n    P(Prompt)-->L(LLM)-->R(Response / Completion);\n```\n\n**The prompt is your primary means of guiding the behavior of the LLM**, so building safe, effective prompts is a vitally important task. The process of building effective prompts is sometimes called *Prompt Engineering*.\n\n# What is Prompt Engineering?\n\n[Prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering) is a collection of techniques for building effective prompts for [Large Language Models](https://en.wikipedia.org/wiki/Large_language_model).\n\nPrompting is a powerful way to adjust a model's behavior to fit your use-case, without the need to make any changes to the model itself.\n\n## Why prompt engineering?\n\nWhat makes prompt engineering such a popular technique for working with LLMs?\n\n- Cost: compared to training a new LLM or fine-tuning an existing one, prompt engineering is a low cost solution\n- Simplicity: prompt engineering can be done without machine learning or coding expertise\n- Flexiblity: prompt engineering can be used to adapt existing (open source or commercial) models to a new use case\n- Speed: prompt engineering is much faster than training a new model or even fine-tuning an existing one\n\n**Bottom line:** prompt engineering is simpler, faster, and cheaper than the other techniques available to change the behavior of LLMs.\n\n## Prompt structure\n\n[This page](https://www.promptingguide.ai/introduction/elements) has an excellent breakdown of the key elements of a prompt.\n\nThe core elements of a prompt are:\n\n1. Instruction: The specific task we want the model to perform.\n1. Context: More information about the context of the task, to help the model produce better output.\n1. Input data: The input to the prompt. This could be input we insert programmatically or it could come from an end-user.\n1. Output Indicator: the type or format of the prompt output.\n\nLet's build an imaginary prompt that includes all these components (we will block off sections with tags `<like this>`, to make it easier to understand the structure:\n\n```xml\n<instruction>\nClassify the text into neutral, positive, or negative\n</instruction>\n\n<context>\nYou are a marketing agent looking at customer's reviews of a restaurant, and evaluating customer sentiment\n</context>\n\n<input-data>\nInput: {user_input}\n</input-data>\n\n<output>\nSentiment: \n</output>\n```\n\nJust imagine that the `<tags>` are not actually present, and you've got the basic idea (although some models *do* allow you to explicitly structure your prompts using XML! Anthropic's [Claude](https://docs.anthropic.com/claude/docs/use-xml-tags) models allow this, for example).\n\nIf we replace `{user_input}` with a restaurant review like `The food was just OK`, the model should respond with a classification. In this case, `Neutral` would probably be an acceptable response.\n\n### How much does the format matter?\n\nShort answer: **a lot**. The size (number of parameters) and type of a model will have a significant impact on how the model responds to a prompt. The same prompt can get very different inputs from different models, and switching from one model to another will nearly always require adjusting your prompt.\n\nMost LLMs also care about **the order of information in the prompt itself**. [This paper]((https://arxiv.org/abs/2307.03172).) shows that **LLMs typically pay more attention to information from the *beginning* and *end* of the prompt**.\n\n**Tip:** Always follow the model creator's guidelines for prompting. Most LLM providers have a set of best practices which should be followed:\n\n- [Anthropic Prompt Engineering Guide](https://docs.anthropic.com/claude/docs/prompt-engineering)\n- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n\n## Prompt engineering methodology\n\nPrompt engineering can seem both overwhelming and very subjective. Are there any general principles we can use to improve prompt quality?\n\n**Yes**. One of the most popular frameworks at present is COSTAR:\n\n- **C**ontext: Information about the task we are asking the LLM to do, and how it should be done.\n- **O**bjective: The purpose (goal) of the request. For instance, we might want to solve a coding issue, or craft a blog post.\n- **S**tyle: The style of writing the LLM should use, for example \"technical\" or \"creative\".\n- **T**one: The tone of the LLM's output, for instance \"casual\", \"friendly\", or \"professional\".\n- **A**udience: Context about the people people the LLM is producing output for.\n- **R**esponse: The end goal the model should be striving for (for example \"accurate\" or \"relevant\" answers)\n\n```xml\n<context>\nHelp create a Facebook post about the benefits of gardening. The focus should be on the health benefits of growing your own food and the psychological benefits of creating a gardening routine\n</context>\n\n<objective>\nCreate a post for me about growing tomato plants from seed in an indoor setting using instructions that even a beginner can follow.\n</objective>\n\n<style>\nUse a friendly, engaging writing style.\n</style>\n\n<audience>\nThe audience is people who are new to gardening but are interested in sustainable living, and health and wellness.\n</audience>\n\n<response>\nThe format should be an accurate, engaging Facebook post.\n</response>\n```\nWithout the `<tags>` removed, here is a response from Anthropic's Claude 3 Opus model:\n\n```\n🌱🍅 Discover the Joys of Growing Tomatoes from Seed Indoors! 🌿🏡\n\nAre you curious about starting your own indoor tomato garden but not sure where to begin? Look no further! Growing tomatoes from seed is a rewarding and fun experience that anyone can enjoy, even if you're new to gardening. 🌱\n\nHere's a simple step-by-step guide to get you started:\n\n1. Choose your tomato seeds. There are many varieties available, so pick one that suits your taste and growing conditions. 🍅\n2. Fill small pots or seed trays with a well-draining seed starting mix.\n3. Plant the seeds about 1/4 inch deep and gently cover them with soil.\n4. Water the soil until it's moist but not soggy, and place the pots in a warm, sunny spot or under grow lights. ☀️\n5. Keep the soil moist and wait for the seeds to germinate, which usually takes 7-14 days.\n6. Once the seedlings have grown a few inches tall and have their first set of true leaves, transplant them into larger pots.\n7. Continue caring for your tomato plants by watering them regularly, providing plenty of light, and fertilizing them every few weeks. 💧\n\nNot only will you enjoy delicious, homegrown tomatoes, but you'll also reap the many health benefits of gardening! Studies show that gardening can reduce stress, improve mental health, and provide gentle physical activity. 💪😊\n\nPlus, by growing your own food, you'll know exactly what goes into it and can enjoy the freshest, most flavorful tomatoes possible. 🍽️\n\nSo why not give indoor tomato gardening a try? Share your experiences and photos in the comments below, and let's inspire each other to cultivate a healthier, more sustainable lifestyle! 🌍💚\n\n#IndoorGardening #TomatoesFromSeed #HealthyLiving #SustainableLiving #GardeningForBeginners\n```\n\nNote that the model provided the emojis, engaging title, and hashtags all on its own. Compare that to the output below, which was generated from the barebones prompt `Create a facebook post about growing tomato plants indoors`:\n\n```\n🍅🌿 Attention all gardening enthusiasts! 🌱\n\nHave you ever wanted to enjoy fresh, juicy tomatoes year-round? Well, I've got great news for you! Growing tomatoes indoors is easier than you might think, and it's a fantastic way to have a constant supply of this delicious fruit. 🍅\n\nHere are a few tips to get you started:\n\n1. Choose the right variety: Opt for smaller, compact tomato plants that are well-suited for indoor growth, such as cherry or plum tomatoes. 🍒\n\n2. Provide plenty of light: Tomatoes need at least 6-8 hours of direct sunlight daily. If you don't have a sunny window, consider using grow lights to ensure your plants receive enough light. ☀️💡\n\n3. Maintain the right temperature: Tomatoes thrive in temperatures between 65-85°F (18-29°C). Keep them away from cold drafts and air conditioning vents. 🌡️\n\n4. Water and fertilize regularly: Keep the soil consistently moist but not soggy, and fertilize your plants every two weeks with a balanced, water-soluble fertilizer. 💦🌿\n\n5. Support your plants: As your tomato plants grow, provide them with a stake, cage, or trellis for support. This will keep the fruits off the ground and improve air circulation. 🪴\n\nGrowing tomatoes indoors is a rewarding experience that allows you to enjoy fresh, homegrown produce no matter the season! 🥗 If you have any tips or success stories about indoor tomato gardening, share them in the comments below! 👇\n\nHappy gardening, everyone! 🌿😊\n\n#IndoorGardening #TomatoPlants #GrowYourOwn #GardeningTips #FreshProduce\n```\n\nStill OK, but note that the prompt does not include specific, targeted information about physical and psychological benefits of gardening, and the instructions are not beginner friendly.\n\n## Approaches to prompting\n\nBeyond ensuring the prompt includes enough detail (by using a framework like CO-STAR), there are also some approaches to prompting that can give the LLM additional context about what we want it to do.\n\n### Zero-shot prompting\n\nA zero-shot prompt does not include any examples of the required response. The prompts we created above using CO-STAR were zero-shot prompts: we did not include any examples of the expected output.\n\nZero-shot prompts are **simple to implement** and **cost-effective**. We use a single, relatively short prompt to produce a single output from the LLM. This reduces our token count, which means we pay less.\n\nHowever zero-shot prompts can be *hard to get right* and can produce **less precise** outputs.\n\nOf course, one way to make an LLM better at zero-shot prompts is by fine-tuning it, as documented [in this paper](https://arxiv.org/abs/2109.01652).\n\nCommercial models like GPT 4 also use RLHF (reinforcement learning with human feedback) to improve performance on zero-shot prompts, as documented [here](https://arxiv.org/abs/1706.03741).\n\n### Few-shot prompting\n\nFew-shot prompts include one or more examples of the model's expected output.\n\nMany LLM creators recommend always using few-shot prompting, because it helps guarantee appropriate output from the model.\n\nHowever, **more work is needed** to craft the prompt (we must decide what examples to include), and **costs will be higher** as the prompt is longer (and therefore contains more tokens, which will lead to higher costs when calling an LLM API that charges per-token).\n\nKey papers, source from [this page](https://www.promptingguide.ai/techniques/fewshot):\n\n- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)\n- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)\n- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n- [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837)\n\n### Chain of Thoughts prompting (CoT)\n\nThis type of prompting helps models to perform complex reasoning through intermediate reasoning steps.\n\nThis improves performance on \"common sense\" reasoning tasks, as well as symbolic and mathematical reasoning.\n\nSome models have specific features to enable this. **Claude** has a [set of xml tags](https://docs.anthropic.com/claude/docs/let-claude-think), that allow you to see the model's reasoning when it responds to CoT prompts:\n\n```xml\n<thinking>The model's thinking will appear here</thinking>\n<answer>The model's answer to the end-user will appear here</answer>\n```\n\nThis can be useful when building a new CoT prompt. Related papers:\n\n- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903), borrowed from [this page](https://www.promptingguide.ai/techniques/cot)\n- [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)\n\n### Variation: Zero-shot CoT\n\nA technique that sometimes works on larger models is simply prompting the model to think out its response. Phrases like \"let's think step by step\" can prompt some models to carry out a chain or reasoning on their own.\n\n### Variation: Auto-CoT\n\nAnother interesting approach proposed in the paper [Automatic Chain of Thought Prompting in Large Language Models](https://arxiv.org/abs/2210.03493), is to generate examples for few-shot CoT prompts automatically using \"think step by step\", then filtering or selecting reasoning examples from the resulting outputs. The code from the paper can be found [here](https://github.com/amazon-science/auto-cot).\n\n### Few-shot with intermediate steps\n\nThis approach lets you combine Chain-of-Thought with few-shot. The model can \"review\" information before answering.\n\nThis can be very helpful when generating code (SQL, JSON). This can be a weak spot for many LLMs because the formatting requirements are so strict.\n\nMore information on few-shot prompting can be found [here](https://www.promptingguide.ai/techniques/fewshot).\n\n","n":0.023}}},{"i":12,"$":{"0":{"v":"Optimization","n":1},"1":{"v":"\nIn the Model Optimization section, we'll focus on tools and techniques for improving:\n\n- The speed at which model training and inference can be performed\n- Techniques for reducing memory and/or compute requirements during training and inference\n- Techniques for optimizing models to run on specific hardware\n\nClick on the links below under **Children** to learn more.\n\n","n":0.137}}},{"i":13,"$":{"0":{"v":"Optimization Techniques","n":0.707},"1":{"v":"\nSome of this is tied to the [[Hardware|ai.optimization.hardware]] section. However we can offer some platform-agnostic general guidance for Hugging Face's Transformers library, as well as TensorFlow and PyTorch. \n\nHardware-specific optimizations (such as [CUDA](https://developer.nvidia.com/cuda-toolkit) for NVIDIA GPUs or [OpenVINO](https://docs.openvino.ai/latest/index.html) for Intel hardware) will be discussed in the Hardware section.\n\n","n":0.144}}},{"i":14,"$":{"0":{"v":"Profiling","n":1},"1":{"v":"\n## Profiling Tools\n\nA discussion of profiling tools which can be used to locate bottlenecks and areas for improvement when optimizing AI workloads.\n","n":0.213}}},{"i":15,"$":{"0":{"v":"Hardware","n":1},"1":{"v":"\nThis section talks about the different hardware configurations which can be used to run ML models. \n\nTo avoid having this section explode to cover every recent version of every piece of hardware from every major vendor, we will try to focus on newer hardware, and on very large models like [SDXL](https://huggingface.co/docs/diffusers/en/using-diffusers/sdxl) or [Llama 2](https://huggingface.co/meta-llama), which are large enough that compute and memory constraints come into play even on new hardware.\n\nFor each piece of hardware we will try to address the following questions:\n\n- What is its general type? \n    - CPU\n    - GPU\n    - FPGA\n    - Something else\n- What is its architecture? \n    - ARM\n    - x86\n    - Something else\n- Where is it available? \n    - Cloud\n        - Which provider?\n        - Which regions?\n    - Local\n- What data types are supported? \n    - FLOAT32\n    - FLOAT16\n    - BFLOAT16\n    - INT8\n    - Others\n- AI Acceleration features? \n    - OpenVINO toolkit (Intel)\n    - Advanced Matrix Extensions (AMX - Intel)\n    - Neuron (AWS Inferentia SDK)\n- What does it cost?\n    - Hourly\n    - Yearly (for providers with up-front pricing options)\n    - Fixed cost (for local hardware)\n- How well does it perform? \n    - Inference and fine-tuning speeds\n    - Inferences-per-second\n    - Cost-per-inference\n\nTo try to keep things organized, we will sort hardware into two general categories:\n\n1. Cloud: hardware available from major cloud providers.\n2. Edge: hardware you run yourself locally.\n\nThis scheme results in some overlap as most of the hardware available through the major cloud providers will overlap, but it means we get to see differences in pricing and regional availability by provider. It also separates out consumer hardware intended to be used and purchased by individuals from server hardware (which is what the cloud companies buy and/or build). \n\n\n\n\n### Edge\n\nNothing here yet!\n\n","n":0.059}}},{"i":16,"$":{"0":{"v":"Cloud Hardware","n":0.707},"1":{"v":"\nClick below to learn more about the hardware various cloud providers make available for running AI models.\n\n","n":0.243}}},{"i":17,"$":{"0":{"v":"GCP (Google Cloud)","n":0.577},"1":{"v":"\nNothing to see here yet!\n\n","n":0.447}}},{"i":18,"$":{"0":{"v":"Microsoft Azure","n":0.707},"1":{"v":"\nNothing to see here yet!","n":0.447}}},{"i":19,"$":{"0":{"v":"Amazon AWS","n":0.707},"1":{"v":"\nAmazon AWS offers a wide range of hardware, including\n\n- Intel and AMD CPUs (x86)\n- FPGAs (Intel and Xilinx)\n- GPUs (NVIDIA and AMD)\n- Trainium (self-built hardware for training models)\n- Inferentia (self-built hardware for running model inference)\n- Graviton (self-built ARM processors)\n\nHere we catalog some of it and provide pricing, benchmarks, and optimization strategies. \n\n# Intel CPUs (x86)\n\nLarge ML models can be run on the CPU using Intel's [OpenVINO toolkit](https://docs.openvino.ai/latest/index.html). \n\nBelow are CPU-only benchmarks on several different EC2 instance types for several different models.\n\nFor each type of hardware listed in the notes, we try to provide tests for at least one popular LLM (such as Llama-7B) and at least one popular text-to-image model (such as SDXL). \n\nTime permitting, benchmarks will also be shown for other models. \n\n","n":0.09}}},{"i":20,"$":{"0":{"v":"HuggingFace","n":1},"1":{"v":"\n# Cool HuggingFace Stuff\n\nStarted this page as a place to jot down quick notes about cool things I find on the Hugging Face website (good pieces of documentation, blog posts, models, leaderboards, and the like). \n\n## Leaderboards\n\n- [OpenLLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)\n\n## Documentation & Tutorials\n\n- [ZeroGPU Spaces](https://huggingface.co/spaces/zero-gpu-explorers/README): how to use ZeroGPU\n- [DreamBooth](https://huggingface.co/docs/diffusers/training/dreambooth): customize diffusion models to produce images of a specific subject\n\n## Prompt Datasets\n\n- [Stable Diffusion Prompt Datasets](https://huggingface.co/datasets?search=stable%20diffusion%20prompt)\n\n## Blog posts\n\n### Overviews and Summaries\n\n- [Text-to-Video: The Task, Challenges and the Current State (May 2023)](https://huggingface.co/blog/text-to-video)\n\n### Understanding Models\n\n- [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)\n\n### Model Usage Techniques\n\n- [Few-shot Learning](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api)\n- [Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts)\n- [Stable Diffusion with 🧨 Diffusers](https://huggingface.co/blog/stable_diffusion)\n\n\n### Quantization and Fine-tuning\n\n- [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)\n- [Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU](https://huggingface.co/blog/trl-peft)\n\n## Models and Spaces\n\n### Translation\n\n- Facebook's [SeamlessM4T v2](https://huggingface.co/facebook/seamless-m4t-v2-large), speech recognition model\n\n### Text-to-Image\n\n- [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev)\n- [FLUX.1 Schnell](https://huggingface.co/black-forest-labs/FLUX.1-schnell)\n\n","n":0.084}}},{"i":21,"$":{"0":{"v":"AI Demos","n":0.707},"1":{"v":"\n# Demo Apps\n\nThis section contains concrete, self-contained demonstrations intended to show how models are trained, fine-tuned, and deployed. These will be toy applications, with the primary focus on the models themselves rather than on practical applications. \n\nThe **goal** will be to provide the apps in a form that is **easy to run** and requires **minimal manual setup**. \n\nApps should be built in a way that either:\n\n1. Enables easy updating even by non-expert users. \n2. Is robust against changes in library or language versions (especially for Python apps).","n":0.107}}},{"i":22,"$":{"0":{"v":"AI Background Knowledge","n":0.577},"1":{"v":"\n# Background Knowledge\n\nThese notes focus on the hows and whys, including:\n\n- What are the different types of ML models and how do they work? \n- What are the tools and libraries for training and deploying ML models? \n- What type(s) of hardware can run these models, and at what level of performance?\n\n","n":0.139}}}]}
