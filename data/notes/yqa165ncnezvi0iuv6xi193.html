<h1 id="amazon-aws">Amazon AWS<a aria-hidden="true" class="anchor-heading icon-link" href="#amazon-aws"></a></h1>
<p>Amazon AWS offers a wide range of hardware, including</p>
<ul>
<li>Intel and AMD CPUs (x86)</li>
<li>FPGAs (Intel and Xilinx)</li>
<li>GPUs (NVIDIA and AMD)</li>
<li>Trainium (self-built hardware for training models)</li>
<li>Inferentia (self-built hardware for running model inference)</li>
<li>Graviton (self-built ARM processors)</li>
</ul>
<p>Here we catalog some of it and provide pricing, benchmarks, and optimization strategies. </p>
<h1 id="intel-cpus-x86">Intel CPUs (x86)<a aria-hidden="true" class="anchor-heading icon-link" href="#intel-cpus-x86"></a></h1>
<p>Large ML models can be run on the CPU using Intel's <a href="https://docs.openvino.ai/latest/index.html">OpenVINO toolkit</a>. </p>
<p>Below are CPU-only benchmarks on several different EC2 instance types for several different models.</p>
<p>For each type of hardware listed in the notes, we try to provide tests for at least one popular LLM (such as Llama-7B) and at least one popular text-to-image model (such as SDXL). </p>
<p>Time permitting, benchmarks will also be shown for other models. </p>