<h1 id="prompt-engineering-generic-notes">Prompt Engineering: Generic Notes<a aria-hidden="true" class="anchor-heading icon-link" href="#prompt-engineering-generic-notes"></a></h1>
<p><strong>Before you read further</strong>: There is already a very comprehensive prompting guide available <a href="https://www.promptingguide.ai/">here</a> from Elvis Saravia on GitHub. If you want an exhaustive intro with more details, check that out! I make frequent references to that guide below. <strong>My notes here borrow extensivley from this guide</strong>, because it's great!</p>
<h1 id="what-is-a-prompt">What is a prompt?<a aria-hidden="true" class="anchor-heading icon-link" href="#what-is-a-prompt"></a></h1>
<p><strong>A prompt is the <em>input</em> to a Large Language Model (LLM)</strong>. The prompt guides the LLM's output (often called a <em>response</em> or <em>completion</em>).</p>
<div class="mermaid">
  flowchart LR;
    P(Prompt)-->L(LLM)-->R(Response / Completion);
</div>
<p><strong>The prompt is your primary means of guiding the behavior of the LLM</strong>, so building safe, effective prompts is a vitally important task. The process of building effective prompts is sometimes called <em>Prompt Engineering</em>.</p>
<h1 id="what-is-prompt-engineering">What is Prompt Engineering?<a aria-hidden="true" class="anchor-heading icon-link" href="#what-is-prompt-engineering"></a></h1>
<p><a href="https://en.wikipedia.org/wiki/Prompt_engineering">Prompt engineering</a> is a collection of techniques for building effective prompts for <a href="https://en.wikipedia.org/wiki/Large_language_model">Large Language Models</a>.</p>
<p>Prompting is a powerful way to adjust a model's behavior to fit your use-case, without the need to make any changes to the model itself.</p>
<h2 id="why-prompt-engineering">Why prompt engineering?<a aria-hidden="true" class="anchor-heading icon-link" href="#why-prompt-engineering"></a></h2>
<p>What makes prompt engineering such a popular technique for working with LLMs?</p>
<ul>
<li>Cost: compared to training a new LLM or fine-tuning an existing one, prompt engineering is a low cost solution</li>
<li>Simplicity: prompt engineering can be done without machine learning or coding expertise</li>
<li>Flexiblity: prompt engineering can be used to adapt existing (open source or commercial) models to a new use case</li>
<li>Speed: prompt engineering is much faster than training a new model or even fine-tuning an existing one</li>
</ul>
<p><strong>Bottom line:</strong> prompt engineering is simpler, faster, and cheaper than the other techniques available to change the behavior of LLMs.</p>
<h2 id="prompt-structure">Prompt structure<a aria-hidden="true" class="anchor-heading icon-link" href="#prompt-structure"></a></h2>
<p><a href="https://www.promptingguide.ai/introduction/elements">This page</a> has an excellent breakdown of the key elements of a prompt.</p>
<p>The core elements of a prompt are:</p>
<ol>
<li>Instruction: The specific task we want the model to perform.</li>
<li>Context: More information about the context of the task, to help the model produce better output.</li>
<li>Input data: The input to the prompt. This could be input we insert programmatically or it could come from an end-user.</li>
<li>Output Indicator: the type or format of the prompt output.</li>
</ol>
<p>Let's build an imaginary prompt that includes all these components (we will block off sections with tags <code>&#x3C;like this></code>, to make it easier to understand the structure:</p>
<pre class="language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>instruction</span><span class="token punctuation">></span></span>
Classify the text into neutral, positive, or negative
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>instruction</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>context</span><span class="token punctuation">></span></span>
You are a marketing agent looking at customer's reviews of a restaurant, and evaluating customer sentiment
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>context</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>input-data</span><span class="token punctuation">></span></span>
Input: {user_input}
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>input-data</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>output</span><span class="token punctuation">></span></span>
Sentiment: 
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>output</span><span class="token punctuation">></span></span>
</code></pre>
<p>Just imagine that the <code>&#x3C;tags></code> are not actually present, and you've got the basic idea (although some models <em>do</em> allow you to explicitly structure your prompts using XML! Anthropic's <a href="https://docs.anthropic.com/claude/docs/use-xml-tags">Claude</a> models allow this, for example).</p>
<p>If we replace <code>{user_input}</code> with a restaurant review like <code>The food was just OK</code>, the model should respond with a classification. In this case, <code>Neutral</code> would probably be an acceptable response.</p>
<h3 id="how-much-does-the-format-matter">How much does the format matter?<a aria-hidden="true" class="anchor-heading icon-link" href="#how-much-does-the-format-matter"></a></h3>
<p>Short answer: <strong>a lot</strong>. The size (number of parameters) and type of a model will have a significant impact on how the model responds to a prompt. The same prompt can get very different inputs from different models, and switching from one model to another will nearly always require adjusting your prompt.</p>
<p>Most LLMs also care about <strong>the order of information in the prompt itself</strong>. <a href="/coding-notes/(https://arxiv.org/abs/2307.03172).">This paper</a> shows that <strong>LLMs typically pay more attention to information from the <em>beginning</em> and <em>end</em> of the prompt</strong>.</p>
<p><strong>Tip:</strong> Always follow the model creator's guidelines for prompting. Most LLM providers have a set of best practices which should be followed:</p>
<ul>
<li><a href="https://docs.anthropic.com/claude/docs/prompt-engineering">Anthropic Prompt Engineering Guide</a></li>
<li><a href="https://platform.openai.com/docs/guides/prompt-engineering">OpenAI Prompt Engineering Guide</a></li>
</ul>
<h2 id="prompt-engineering-methodology">Prompt engineering methodology<a aria-hidden="true" class="anchor-heading icon-link" href="#prompt-engineering-methodology"></a></h2>
<p>Prompt engineering can seem both overwhelming and very subjective. Are there any general principles we can use to improve prompt quality?</p>
<p><strong>Yes</strong>. One of the most popular frameworks at present is COSTAR:</p>
<ul>
<li><strong>C</strong>ontext: Information about the task we are asking the LLM to do, and how it should be done.</li>
<li><strong>O</strong>bjective: The purpose (goal) of the request. For instance, we might want to solve a coding issue, or craft a blog post.</li>
<li><strong>S</strong>tyle: The style of writing the LLM should use, for example "technical" or "creative".</li>
<li><strong>T</strong>one: The tone of the LLM's output, for instance "casual", "friendly", or "professional".</li>
<li><strong>A</strong>udience: Context about the people people the LLM is producing output for.</li>
<li><strong>R</strong>esponse: The end goal the model should be striving for (for example "accurate" or "relevant" answers)</li>
</ul>
<pre class="language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>context</span><span class="token punctuation">></span></span>
Help create a Facebook post about the benefits of gardening. The focus should be on the health benefits of growing your own food and the psychological benefits of creating a gardening routine
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>context</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>objective</span><span class="token punctuation">></span></span>
Create a post for me about growing tomato plants from seed in an indoor setting using instructions that even a beginner can follow.
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>objective</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>style</span><span class="token punctuation">></span></span>
Use a friendly, engaging writing style.
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>style</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>audience</span><span class="token punctuation">></span></span>
The audience is people who are new to gardening but are interested in sustainable living, and health and wellness.
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>audience</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>response</span><span class="token punctuation">></span></span>
The format should be an accurate, engaging Facebook post.
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>response</span><span class="token punctuation">></span></span>
</code></pre>
<p>Without the <code>&#x3C;tags></code> removed, here is a response from Anthropic's Claude 3 Opus model:</p>
<pre><code>üå±üçÖ Discover the Joys of Growing Tomatoes from Seed Indoors! üåøüè°

Are you curious about starting your own indoor tomato garden but not sure where to begin? Look no further! Growing tomatoes from seed is a rewarding and fun experience that anyone can enjoy, even if you're new to gardening. üå±

Here's a simple step-by-step guide to get you started:

1. Choose your tomato seeds. There are many varieties available, so pick one that suits your taste and growing conditions. üçÖ
2. Fill small pots or seed trays with a well-draining seed starting mix.
3. Plant the seeds about 1/4 inch deep and gently cover them with soil.
4. Water the soil until it's moist but not soggy, and place the pots in a warm, sunny spot or under grow lights. ‚òÄÔ∏è
5. Keep the soil moist and wait for the seeds to germinate, which usually takes 7-14 days.
6. Once the seedlings have grown a few inches tall and have their first set of true leaves, transplant them into larger pots.
7. Continue caring for your tomato plants by watering them regularly, providing plenty of light, and fertilizing them every few weeks. üíß

Not only will you enjoy delicious, homegrown tomatoes, but you'll also reap the many health benefits of gardening! Studies show that gardening can reduce stress, improve mental health, and provide gentle physical activity. üí™üòä

Plus, by growing your own food, you'll know exactly what goes into it and can enjoy the freshest, most flavorful tomatoes possible. üçΩÔ∏è

So why not give indoor tomato gardening a try? Share your experiences and photos in the comments below, and let's inspire each other to cultivate a healthier, more sustainable lifestyle! üåçüíö

#IndoorGardening #TomatoesFromSeed #HealthyLiving #SustainableLiving #GardeningForBeginners
</code></pre>
<p>Note that the model provided the emojis, engaging title, and hashtags all on its own. Compare that to the output below, which was generated from the barebones prompt <code>Create a facebook post about growing tomato plants indoors</code>:</p>
<pre><code>üçÖüåø Attention all gardening enthusiasts! üå±

Have you ever wanted to enjoy fresh, juicy tomatoes year-round? Well, I've got great news for you! Growing tomatoes indoors is easier than you might think, and it's a fantastic way to have a constant supply of this delicious fruit. üçÖ

Here are a few tips to get you started:

1. Choose the right variety: Opt for smaller, compact tomato plants that are well-suited for indoor growth, such as cherry or plum tomatoes. üçí

2. Provide plenty of light: Tomatoes need at least 6-8 hours of direct sunlight daily. If you don't have a sunny window, consider using grow lights to ensure your plants receive enough light. ‚òÄÔ∏èüí°

3. Maintain the right temperature: Tomatoes thrive in temperatures between 65-85¬∞F (18-29¬∞C). Keep them away from cold drafts and air conditioning vents. üå°Ô∏è

4. Water and fertilize regularly: Keep the soil consistently moist but not soggy, and fertilize your plants every two weeks with a balanced, water-soluble fertilizer. üí¶üåø

5. Support your plants: As your tomato plants grow, provide them with a stake, cage, or trellis for support. This will keep the fruits off the ground and improve air circulation. ü™¥

Growing tomatoes indoors is a rewarding experience that allows you to enjoy fresh, homegrown produce no matter the season! ü•ó If you have any tips or success stories about indoor tomato gardening, share them in the comments below! üëá

Happy gardening, everyone! üåøüòä

#IndoorGardening #TomatoPlants #GrowYourOwn #GardeningTips #FreshProduce
</code></pre>
<p>Still OK, but note that the prompt does not include specific, targeted information about physical and psychological benefits of gardening, and the instructions are not beginner friendly.</p>
<h2 id="approaches-to-prompting">Approaches to prompting<a aria-hidden="true" class="anchor-heading icon-link" href="#approaches-to-prompting"></a></h2>
<p>Beyond ensuring the prompt includes enough detail (by using a framework like CO-STAR), there are also some approaches to prompting that can give the LLM additional context about what we want it to do.</p>
<h3 id="zero-shot-prompting">Zero-shot prompting<a aria-hidden="true" class="anchor-heading icon-link" href="#zero-shot-prompting"></a></h3>
<p>A zero-shot prompt does not include any examples of the required response. The prompts we created above using CO-STAR were zero-shot prompts: we did not include any examples of the expected output.</p>
<p>Zero-shot prompts are <strong>simple to implement</strong> and <strong>cost-effective</strong>. We use a single, relatively short prompt to produce a single output from the LLM. This reduces our token count, which means we pay less.</p>
<p>However zero-shot prompts can be <em>hard to get right</em> and can produce <strong>less precise</strong> outputs.</p>
<p>Of course, one way to make an LLM better at zero-shot prompts is by fine-tuning it, as documented <a href="https://arxiv.org/abs/2109.01652">in this paper</a>.</p>
<p>Commercial models like GPT 4 also use RLHF (reinforcement learning with human feedback) to improve performance on zero-shot prompts, as documented <a href="https://arxiv.org/abs/1706.03741">here</a>.</p>
<h3 id="few-shot-prompting">Few-shot prompting<a aria-hidden="true" class="anchor-heading icon-link" href="#few-shot-prompting"></a></h3>
<p>Few-shot prompts include one or more examples of the model's expected output.</p>
<p>Many LLM creators recommend always using few-shot prompting, because it helps guarantee appropriate output from the model.</p>
<p>However, <strong>more work is needed</strong> to craft the prompt (we must decide what examples to include), and <strong>costs will be higher</strong> as the prompt is longer (and therefore contains more tokens, which will lead to higher costs when calling an LLM API that charges per-token).</p>
<p>Key papers, source from <a href="https://www.promptingguide.ai/techniques/fewshot">this page</a>:</p>
<ul>
<li><a href="https://arxiv.org/abs/2302.13971">LLaMA: Open and Efficient Foundation Language Models</a></li>
<li><a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a></li>
<li><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a></li>
<li><a href="https://arxiv.org/abs/2202.12837">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</a></li>
</ul>
<h3 id="chain-of-thoughts-prompting-cot">Chain of Thoughts prompting (CoT)<a aria-hidden="true" class="anchor-heading icon-link" href="#chain-of-thoughts-prompting-cot"></a></h3>
<p>This type of prompting helps models to perform complex reasoning through intermediate reasoning steps.</p>
<p>This improves performance on "common sense" reasoning tasks, as well as symbolic and mathematical reasoning.</p>
<p>Some models have specific features to enable this. <strong>Claude</strong> has a <a href="https://docs.anthropic.com/claude/docs/let-claude-think">set of xml tags</a>, that allow you to see the model's reasoning when it responds to CoT prompts:</p>
<pre class="language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>thinking</span><span class="token punctuation">></span></span>The model's thinking will appear here<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>thinking</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>answer</span><span class="token punctuation">></span></span>The model's answer to the end-user will appear here<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>answer</span><span class="token punctuation">></span></span>
</code></pre>
<p>This can be useful when building a new CoT prompt. Related papers:</p>
<ul>
<li><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a>, borrowed from <a href="https://www.promptingguide.ai/techniques/cot">this page</a></li>
<li><a href="https://arxiv.org/abs/2205.11916">Large Language Models are Zero-Shot Reasoners</a></li>
</ul>
<h3 id="variation-zero-shot-cot">Variation: Zero-shot CoT<a aria-hidden="true" class="anchor-heading icon-link" href="#variation-zero-shot-cot"></a></h3>
<p>A technique that sometimes works on larger models is simply prompting the model to think out its response. Phrases like "let's think step by step" can prompt some models to carry out a chain or reasoning on their own.</p>
<h3 id="variation-auto-cot">Variation: Auto-CoT<a aria-hidden="true" class="anchor-heading icon-link" href="#variation-auto-cot"></a></h3>
<p>Another interesting approach proposed in the paper <a href="https://arxiv.org/abs/2210.03493">Automatic Chain of Thought Prompting in Large Language Models</a>, is to generate examples for few-shot CoT prompts automatically using "think step by step", then filtering or selecting reasoning examples from the resulting outputs. The code from the paper can be found <a href="https://github.com/amazon-science/auto-cot">here</a>.</p>
<h3 id="few-shot-with-intermediate-steps">Few-shot with intermediate steps<a aria-hidden="true" class="anchor-heading icon-link" href="#few-shot-with-intermediate-steps"></a></h3>
<p>This approach lets you combine Chain-of-Thought with few-shot. The model can "review" information before answering.</p>
<p>This can be very helpful when generating code (SQL, JSON). This can be a weak spot for many LLMs because the formatting requirements are so strict.</p>
<p>More information on few-shot prompting can be found <a href="https://www.promptingguide.ai/techniques/fewshot">here</a>.</p>