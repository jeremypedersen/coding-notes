<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/coding-notes/favicon.ico"/><title>Prompt Engineering: Generic Notes</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Prompt Engineering: Generic Notes"/><meta property="og:title" content="Prompt Engineering: Generic Notes"/><meta property="og:description" content="Prompt Engineering: Generic Notes"/><meta property="og:url" content="https://jeremypedersen.github.io/coding-notes/notes/qtq92f3bsp0mds91svqsxnx/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="4/15/2024"/><meta property="article:modified_time" content="4/16/2024"/><link rel="canonical" href="https://jeremypedersen.github.io/coding-notes/notes/qtq92f3bsp0mds91svqsxnx/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/coding-notes/_next/static/css/b0c7e77ce5f99436.css" as="style"/><link rel="stylesheet" href="/coding-notes/_next/static/css/b0c7e77ce5f99436.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/coding-notes/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/coding-notes/_next/static/chunks/webpack-eed3f0eedf802397.js" defer=""></script><script src="/coding-notes/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/coding-notes/_next/static/chunks/main-29387b2bb14b1eb7.js" defer=""></script><script src="/coding-notes/_next/static/chunks/pages/_app-dc4bb01b71d58e47.js" defer=""></script><script src="/coding-notes/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/coding-notes/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/coding-notes/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/coding-notes/_next/static/jdQmoR0T7zxn3zBS6JrtA/_buildManifest.js" defer=""></script><script src="/coding-notes/_next/static/jdQmoR0T7zxn3zBS6JrtA/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="prompt-engineering-generic-notes">Prompt Engineering: Generic Notes<a aria-hidden="true" class="anchor-heading icon-link" href="#prompt-engineering-generic-notes"></a></h1>
<p><strong>Before you read further</strong>: There is already a very comprehensive prompting guide available <a href="https://www.promptingguide.ai/">here</a> from Elvis Saravia on GitHub. If you want an exhaustive intro with more details, check that out! I make frequent references to that guide below. <strong>My notes here borrow extensivley from this guide</strong>, because it's great!</p>
<h1 id="what-is-a-prompt">What is a prompt?<a aria-hidden="true" class="anchor-heading icon-link" href="#what-is-a-prompt"></a></h1>
<p><strong>A prompt is the <em>input</em> to a Large Language Model (LLM)</strong>. The prompt guides the LLM's output (often called a <em>response</em> or <em>completion</em>).</p>
<div class="mermaid">
  flowchart LR;
    P(Prompt)-->L(LLM)-->R(Response / Completion);
</div>
<p><strong>The prompt is your primary means of guiding the behavior of the LLM</strong>, so building safe, effective prompts is a vitally important task. The process of building effective prompts is sometimes called <em>Prompt Engineering</em>.</p>
<h1 id="what-is-prompt-engineering">What is Prompt Engineering?<a aria-hidden="true" class="anchor-heading icon-link" href="#what-is-prompt-engineering"></a></h1>
<p><a href="https://en.wikipedia.org/wiki/Prompt_engineering">Prompt engineering</a> is a collection of techniques for building effective prompts for <a href="https://en.wikipedia.org/wiki/Large_language_model">Large Language Models</a>.</p>
<p>Prompting is a powerful way to adjust a model's behavior to fit your use-case, without the need to make any changes to the model itself.</p>
<h2 id="why-prompt-engineering">Why prompt engineering?<a aria-hidden="true" class="anchor-heading icon-link" href="#why-prompt-engineering"></a></h2>
<p>What makes prompt engineering such a popular technique for working with LLMs?</p>
<ul>
<li>Cost: compared to training a new LLM or fine-tuning an existing one, prompt engineering is a low cost solution</li>
<li>Simplicity: prompt engineering can be done without machine learning or coding expertise</li>
<li>Flexiblity: prompt engineering can be used to adapt existing (open source or commercial) models to a new use case</li>
<li>Speed: prompt engineering is much faster than training a new model or even fine-tuning an existing one</li>
</ul>
<p><strong>Bottom line:</strong> prompt engineering is simpler, faster, and cheaper than the other techniques available to change the behavior of LLMs.</p>
<h2 id="prompt-structure">Prompt structure<a aria-hidden="true" class="anchor-heading icon-link" href="#prompt-structure"></a></h2>
<p><a href="https://www.promptingguide.ai/introduction/elements">This page</a> has an excellent breakdown of the key elements of a prompt.</p>
<p>The core elements of a prompt are:</p>
<ol>
<li>Instruction: The specific task we want the model to perform.</li>
<li>Context: More information about the context of the task, to help the model produce better output.</li>
<li>Input data: The input to the prompt. This could be input we insert programmatically or it could come from an end-user.</li>
<li>Output Indicator: the type or format of the prompt output.</li>
</ol>
<p>Let's build an imaginary prompt that includes all these components (we will block off sections with tags <code>&#x3C;like this></code>, to make it easier to understand the structure:</p>
<pre class="language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>instruction</span><span class="token punctuation">></span></span>
Classify the text into neutral, positive, or negative
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>instruction</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>context</span><span class="token punctuation">></span></span>
You are a marketing agent looking at customer's reviews of a restaurant, and evaluating customer sentiment
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>context</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>input-data</span><span class="token punctuation">></span></span>
Input: {user_input}
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>input-data</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>output</span><span class="token punctuation">></span></span>
Sentiment: 
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>output</span><span class="token punctuation">></span></span>
</code></pre>
<p>Just imagine that the <code>&#x3C;tags></code> are not actually present, and you've got the basic idea (although some models <em>do</em> allow you to explicitly structure your prompts using XML! Anthropic's <a href="https://docs.anthropic.com/claude/docs/use-xml-tags">Claude</a> models allow this, for example).</p>
<p>If we replace <code>{user_input}</code> with a restaurant review like <code>The food was just OK</code>, the model should respond with a classification. In this case, <code>Neutral</code> would probably be an acceptable response.</p>
<h3 id="how-much-does-the-format-matter">How much does the format matter?<a aria-hidden="true" class="anchor-heading icon-link" href="#how-much-does-the-format-matter"></a></h3>
<p>Short answer: <strong>a lot</strong>. The size (number of parameters) and type of a model will have a significant impact on how the model responds to a prompt. The same prompt can get very different inputs from different models, and switching from one model to another will nearly always require adjusting your prompt.</p>
<p>Most LLMs also care about <strong>the order of information in the prompt itself</strong>. <a href="/coding-notes/(https://arxiv.org/abs/2307.03172).">This paper</a> shows that <strong>LLMs typically pay more attention to information from the <em>beginning</em> and <em>end</em> of the prompt</strong>.</p>
<p><strong>Tip:</strong> Always follow the model creator's guidelines for prompting. Most LLM providers have a set of best practices which should be followed:</p>
<ul>
<li><a href="https://docs.anthropic.com/claude/docs/prompt-engineering">Anthropic Prompt Engineering Guide</a></li>
<li><a href="https://platform.openai.com/docs/guides/prompt-engineering">OpenAI Prompt Engineering Guide</a></li>
</ul>
<h2 id="prompt-engineering-methodology">Prompt engineering methodology<a aria-hidden="true" class="anchor-heading icon-link" href="#prompt-engineering-methodology"></a></h2>
<p>Prompt engineering can seem both overwhelming and very subjective. Are there any general principles we can use to improve prompt quality?</p>
<p><strong>Yes</strong>. One of the most popular frameworks at present is COSTAR:</p>
<ul>
<li><strong>C</strong>ontext: Information about the task we are asking the LLM to do, and how it should be done.</li>
<li><strong>O</strong>bjective: The purpose (goal) of the request. For instance, we might want to solve a coding issue, or craft a blog post.</li>
<li><strong>S</strong>tyle: The style of writing the LLM should use, for example "technical" or "creative".</li>
<li><strong>T</strong>one: The tone of the LLM's output, for instance "casual", "friendly", or "professional".</li>
<li><strong>A</strong>udience: Context about the people people the LLM is producing output for.</li>
<li><strong>R</strong>esponse: The end goal the model should be striving for (for example "accurate" or "relevant" answers)</li>
</ul>
<pre class="language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>context</span><span class="token punctuation">></span></span>
Help create a Facebook post about the benefits of gardening. The focus should be on the health benefits of growing your own food and the psychological benefits of creating a gardening routine
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>context</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>objective</span><span class="token punctuation">></span></span>
Create a post for me about growing tomato plants from seed in an indoor setting using instructions that even a beginner can follow.
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>objective</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>style</span><span class="token punctuation">></span></span>
Use a friendly, engaging writing style.
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>style</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>audience</span><span class="token punctuation">></span></span>
The audience is people who are new to gardening but are interested in sustainable living, and health and wellness.
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>audience</span><span class="token punctuation">></span></span>

<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>response</span><span class="token punctuation">></span></span>
The format should be an accurate, engaging Facebook post.
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>response</span><span class="token punctuation">></span></span>
</code></pre>
<p>Without the <code>&#x3C;tags></code> removed, here is a response from Anthropic's Claude 3 Opus model:</p>
<pre><code>🌱🍅 Discover the Joys of Growing Tomatoes from Seed Indoors! 🌿🏡

Are you curious about starting your own indoor tomato garden but not sure where to begin? Look no further! Growing tomatoes from seed is a rewarding and fun experience that anyone can enjoy, even if you're new to gardening. 🌱

Here's a simple step-by-step guide to get you started:

1. Choose your tomato seeds. There are many varieties available, so pick one that suits your taste and growing conditions. 🍅
2. Fill small pots or seed trays with a well-draining seed starting mix.
3. Plant the seeds about 1/4 inch deep and gently cover them with soil.
4. Water the soil until it's moist but not soggy, and place the pots in a warm, sunny spot or under grow lights. ☀️
5. Keep the soil moist and wait for the seeds to germinate, which usually takes 7-14 days.
6. Once the seedlings have grown a few inches tall and have their first set of true leaves, transplant them into larger pots.
7. Continue caring for your tomato plants by watering them regularly, providing plenty of light, and fertilizing them every few weeks. 💧

Not only will you enjoy delicious, homegrown tomatoes, but you'll also reap the many health benefits of gardening! Studies show that gardening can reduce stress, improve mental health, and provide gentle physical activity. 💪😊

Plus, by growing your own food, you'll know exactly what goes into it and can enjoy the freshest, most flavorful tomatoes possible. 🍽️

So why not give indoor tomato gardening a try? Share your experiences and photos in the comments below, and let's inspire each other to cultivate a healthier, more sustainable lifestyle! 🌍💚

#IndoorGardening #TomatoesFromSeed #HealthyLiving #SustainableLiving #GardeningForBeginners
</code></pre>
<p>Note that the model provided the emojis, engaging title, and hashtags all on its own. Compare that to the output below, which was generated from the barebones prompt <code>Create a facebook post about growing tomato plants indoors</code>:</p>
<pre><code>🍅🌿 Attention all gardening enthusiasts! 🌱

Have you ever wanted to enjoy fresh, juicy tomatoes year-round? Well, I've got great news for you! Growing tomatoes indoors is easier than you might think, and it's a fantastic way to have a constant supply of this delicious fruit. 🍅

Here are a few tips to get you started:

1. Choose the right variety: Opt for smaller, compact tomato plants that are well-suited for indoor growth, such as cherry or plum tomatoes. 🍒

2. Provide plenty of light: Tomatoes need at least 6-8 hours of direct sunlight daily. If you don't have a sunny window, consider using grow lights to ensure your plants receive enough light. ☀️💡

3. Maintain the right temperature: Tomatoes thrive in temperatures between 65-85°F (18-29°C). Keep them away from cold drafts and air conditioning vents. 🌡️

4. Water and fertilize regularly: Keep the soil consistently moist but not soggy, and fertilize your plants every two weeks with a balanced, water-soluble fertilizer. 💦🌿

5. Support your plants: As your tomato plants grow, provide them with a stake, cage, or trellis for support. This will keep the fruits off the ground and improve air circulation. 🪴

Growing tomatoes indoors is a rewarding experience that allows you to enjoy fresh, homegrown produce no matter the season! 🥗 If you have any tips or success stories about indoor tomato gardening, share them in the comments below! 👇

Happy gardening, everyone! 🌿😊

#IndoorGardening #TomatoPlants #GrowYourOwn #GardeningTips #FreshProduce
</code></pre>
<p>Still OK, but note that the prompt does not include specific, targeted information about physical and psychological benefits of gardening, and the instructions are not beginner friendly.</p>
<h2 id="approaches-to-prompting">Approaches to prompting<a aria-hidden="true" class="anchor-heading icon-link" href="#approaches-to-prompting"></a></h2>
<p>Beyond ensuring the prompt includes enough detail (by using a framework like CO-STAR), there are also some approaches to prompting that can give the LLM additional context about what we want it to do.</p>
<h3 id="zero-shot-prompting">Zero-shot prompting<a aria-hidden="true" class="anchor-heading icon-link" href="#zero-shot-prompting"></a></h3>
<p>A zero-shot prompt does not include any examples of the required response. The prompts we created above using CO-STAR were zero-shot prompts: we did not include any examples of the expected output.</p>
<p>Zero-shot prompts are <strong>simple to implement</strong> and <strong>cost-effective</strong>. We use a single, relatively short prompt to produce a single output from the LLM. This reduces our token count, which means we pay less.</p>
<p>However zero-shot prompts can be <em>hard to get right</em> and can produce <strong>less precise</strong> outputs.</p>
<p>Of course, one way to make an LLM better at zero-shot prompts is by fine-tuning it, as documented <a href="https://arxiv.org/abs/2109.01652">in this paper</a>.</p>
<p>Commercial models like GPT 4 also use RLHF (reinforcement learning with human feedback) to improve performance on zero-shot prompts, as documented <a href="https://arxiv.org/abs/1706.03741">here</a>.</p>
<h3 id="few-shot-prompting">Few-shot prompting<a aria-hidden="true" class="anchor-heading icon-link" href="#few-shot-prompting"></a></h3>
<p>Few-shot prompts include one or more examples of the model's expected output.</p>
<p>Many LLM creators recommend always using few-shot prompting, because it helps guarantee appropriate output from the model.</p>
<p>However, <strong>more work is needed</strong> to craft the prompt (we must decide what examples to include), and <strong>costs will be higher</strong> as the prompt is longer (and therefore contains more tokens, which will lead to higher costs when calling an LLM API that charges per-token).</p>
<p>Key papers, source from <a href="https://www.promptingguide.ai/techniques/fewshot">this page</a>:</p>
<ul>
<li><a href="https://arxiv.org/abs/2302.13971">LLaMA: Open and Efficient Foundation Language Models</a></li>
<li><a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a></li>
<li><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a></li>
<li><a href="https://arxiv.org/abs/2202.12837">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</a></li>
</ul>
<h3 id="chain-of-thoughts-prompting-cot">Chain of Thoughts prompting (CoT)<a aria-hidden="true" class="anchor-heading icon-link" href="#chain-of-thoughts-prompting-cot"></a></h3>
<p>This type of prompting helps models to perform complex reasoning through intermediate reasoning steps.</p>
<p>This improves performance on "common sense" reasoning tasks, as well as symbolic and mathematical reasoning.</p>
<p>Some models have specific features to enable this. <strong>Claude</strong> has a <a href="https://docs.anthropic.com/claude/docs/let-claude-think">set of xml tags</a>, that allow you to see the model's reasoning when it responds to CoT prompts:</p>
<pre class="language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>thinking</span><span class="token punctuation">></span></span>The model's thinking will appear here<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>thinking</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;</span>answer</span><span class="token punctuation">></span></span>The model's answer to the end-user will appear here<span class="token tag"><span class="token tag"><span class="token punctuation">&#x3C;/</span>answer</span><span class="token punctuation">></span></span>
</code></pre>
<p>This can be useful when building a new CoT prompt. Related papers:</p>
<ul>
<li><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a>, borrowed from <a href="https://www.promptingguide.ai/techniques/cot">this page</a></li>
<li><a href="https://arxiv.org/abs/2205.11916">Large Language Models are Zero-Shot Reasoners</a></li>
</ul>
<h3 id="variation-zero-shot-cot">Variation: Zero-shot CoT<a aria-hidden="true" class="anchor-heading icon-link" href="#variation-zero-shot-cot"></a></h3>
<p>A technique that sometimes works on larger models is simply prompting the model to think out its response. Phrases like "let's think step by step" can prompt some models to carry out a chain or reasoning on their own.</p>
<h3 id="variation-auto-cot">Variation: Auto-CoT<a aria-hidden="true" class="anchor-heading icon-link" href="#variation-auto-cot"></a></h3>
<p>Another interesting approach proposed in the paper <a href="https://arxiv.org/abs/2210.03493">Automatic Chain of Thought Prompting in Large Language Models</a>, is to generate examples for few-shot CoT prompts automatically using "think step by step", then filtering or selecting reasoning examples from the resulting outputs. The code from the paper can be found <a href="https://github.com/amazon-science/auto-cot">here</a>.</p>
<h3 id="few-shot-with-intermediate-steps">Few-shot with intermediate steps<a aria-hidden="true" class="anchor-heading icon-link" href="#few-shot-with-intermediate-steps"></a></h3>
<p>This approach lets you combine Chain-of-Thought with few-shot. The model can "review" information before answering.</p>
<p>This can be very helpful when generating code (SQL, JSON). This can be a weak spot for many LLMs because the formatting requirements are so strict.</p>
<p>More information on few-shot prompting can be found <a href="https://www.promptingguide.ai/techniques/fewshot">here</a>.</p></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#what-is-a-prompt" title="What is a prompt?">What is a prompt?</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#what-is-prompt-engineering" title="What is Prompt Engineering?">What is Prompt Engineering?</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#why-prompt-engineering" title="Why prompt engineering?">Why prompt engineering?</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#prompt-structure" title="Prompt structure">Prompt structure</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#how-much-does-the-format-matter" title="How much does the format matter?">How much does the format matter?</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#prompt-engineering-methodology" title="Prompt engineering methodology">Prompt engineering methodology</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#approaches-to-prompting" title="Approaches to prompting">Approaches to prompting</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#zero-shot-prompting" title="Zero-shot prompting">Zero-shot prompting</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#few-shot-prompting" title="Few-shot prompting">Few-shot prompting</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#chain-of-thoughts-prompting-cot" title="Chain of Thoughts prompting (CoT)">Chain of Thoughts prompting (CoT)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#variation-zero-shot-cot" title="Variation: Zero-shot CoT">Variation: Zero-shot CoT</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#variation-auto-cot" title="Variation: Auto-CoT">Variation: Auto-CoT</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#few-shot-with-intermediate-steps" title="Few-shot with intermediate steps">Few-shot with intermediate steps</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"qtq92f3bsp0mds91svqsxnx","title":"Prompt Engineering: Generic Notes","desc":"Prompt Engineering: Generic Notes","updated":1713239384797,"created":1713160931622,"custom":{},"fname":"ai.prompting.llms.generic","type":"note","vault":{"fsPath":".","selfContained":true,"name":"coding-notes"},"contentHash":"7f4a3e10d09267b364aa85620eb296fe","links":[],"anchors":{"what-is-a-prompt":{"type":"header","text":"What is a prompt?","value":"what-is-a-prompt","line":10,"column":0,"depth":1},"what-is-prompt-engineering":{"type":"header","text":"What is Prompt Engineering?","value":"what-is-prompt-engineering","line":21,"column":0,"depth":1},"why-prompt-engineering":{"type":"header","text":"Why prompt engineering?","value":"why-prompt-engineering","line":27,"column":0,"depth":2},"prompt-structure":{"type":"header","text":"Prompt structure","value":"prompt-structure","line":38,"column":0,"depth":2},"how-much-does-the-format-matter":{"type":"header","text":"How much does the format matter?","value":"how-much-does-the-format-matter","line":73,"column":0,"depth":3},"prompt-engineering-methodology":{"type":"header","text":"Prompt engineering methodology","value":"prompt-engineering-methodology","line":84,"column":0,"depth":2},"approaches-to-prompting":{"type":"header","text":"Approaches to prompting","value":"approaches-to-prompting","line":172,"column":0,"depth":2},"zero-shot-prompting":{"type":"header","text":"Zero-shot prompting","value":"zero-shot-prompting","line":176,"column":0,"depth":3},"few-shot-prompting":{"type":"header","text":"Few-shot prompting","value":"few-shot-prompting","line":188,"column":0,"depth":3},"chain-of-thoughts-prompting-cot":{"type":"header","text":"Chain of Thoughts prompting (CoT)","value":"chain-of-thoughts-prompting-cot","line":203,"column":0,"depth":3},"variation-zero-shot-cot":{"type":"header","text":"Variation: Zero-shot CoT","value":"variation-zero-shot-cot","line":221,"column":0,"depth":3},"variation-auto-cot":{"type":"header","text":"Variation: Auto-CoT","value":"variation-auto-cot","line":225,"column":0,"depth":3},"few-shot-with-intermediate-steps":{"type":"header","text":"Few-shot with intermediate steps","value":"few-shot-with-intermediate-steps","line":229,"column":0,"depth":3}},"children":[],"parent":"re9huq8oxr122kob0etdd6z","data":{}},"body":"\u003ch1 id=\"prompt-engineering-generic-notes\"\u003ePrompt Engineering: Generic Notes\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#prompt-engineering-generic-notes\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eBefore you read further\u003c/strong\u003e: There is already a very comprehensive prompting guide available \u003ca href=\"https://www.promptingguide.ai/\"\u003ehere\u003c/a\u003e from Elvis Saravia on GitHub. If you want an exhaustive intro with more details, check that out! I make frequent references to that guide below. \u003cstrong\u003eMy notes here borrow extensivley from this guide\u003c/strong\u003e, because it's great!\u003c/p\u003e\n\u003ch1 id=\"what-is-a-prompt\"\u003eWhat is a prompt?\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#what-is-a-prompt\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eA prompt is the \u003cem\u003einput\u003c/em\u003e to a Large Language Model (LLM)\u003c/strong\u003e. The prompt guides the LLM's output (often called a \u003cem\u003eresponse\u003c/em\u003e or \u003cem\u003ecompletion\u003c/em\u003e).\u003c/p\u003e\n\u003cdiv class=\"mermaid\"\u003e\n  flowchart LR;\n    P(Prompt)--\u003eL(LLM)--\u003eR(Response / Completion);\n\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003eThe prompt is your primary means of guiding the behavior of the LLM\u003c/strong\u003e, so building safe, effective prompts is a vitally important task. The process of building effective prompts is sometimes called \u003cem\u003ePrompt Engineering\u003c/em\u003e.\u003c/p\u003e\n\u003ch1 id=\"what-is-prompt-engineering\"\u003eWhat is Prompt Engineering?\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#what-is-prompt-engineering\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Prompt_engineering\"\u003ePrompt engineering\u003c/a\u003e is a collection of techniques for building effective prompts for \u003ca href=\"https://en.wikipedia.org/wiki/Large_language_model\"\u003eLarge Language Models\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003ePrompting is a powerful way to adjust a model's behavior to fit your use-case, without the need to make any changes to the model itself.\u003c/p\u003e\n\u003ch2 id=\"why-prompt-engineering\"\u003eWhy prompt engineering?\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#why-prompt-engineering\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWhat makes prompt engineering such a popular technique for working with LLMs?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCost: compared to training a new LLM or fine-tuning an existing one, prompt engineering is a low cost solution\u003c/li\u003e\n\u003cli\u003eSimplicity: prompt engineering can be done without machine learning or coding expertise\u003c/li\u003e\n\u003cli\u003eFlexiblity: prompt engineering can be used to adapt existing (open source or commercial) models to a new use case\u003c/li\u003e\n\u003cli\u003eSpeed: prompt engineering is much faster than training a new model or even fine-tuning an existing one\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eBottom line:\u003c/strong\u003e prompt engineering is simpler, faster, and cheaper than the other techniques available to change the behavior of LLMs.\u003c/p\u003e\n\u003ch2 id=\"prompt-structure\"\u003ePrompt structure\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#prompt-structure\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.promptingguide.ai/introduction/elements\"\u003eThis page\u003c/a\u003e has an excellent breakdown of the key elements of a prompt.\u003c/p\u003e\n\u003cp\u003eThe core elements of a prompt are:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eInstruction: The specific task we want the model to perform.\u003c/li\u003e\n\u003cli\u003eContext: More information about the context of the task, to help the model produce better output.\u003c/li\u003e\n\u003cli\u003eInput data: The input to the prompt. This could be input we insert programmatically or it could come from an end-user.\u003c/li\u003e\n\u003cli\u003eOutput Indicator: the type or format of the prompt output.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eLet's build an imaginary prompt that includes all these components (we will block off sections with tags \u003ccode\u003e\u0026#x3C;like this\u003e\u003c/code\u003e, to make it easier to understand the structure:\u003c/p\u003e\n\u003cpre class=\"language-xml\"\u003e\u003ccode class=\"language-xml\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;\u003c/span\u003einstruction\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\nClassify the text into neutral, positive, or negative\n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;/\u003c/span\u003einstruction\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;\u003c/span\u003econtext\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\nYou are a marketing agent looking at customer's reviews of a restaurant, and evaluating customer sentiment\n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;/\u003c/span\u003econtext\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;\u003c/span\u003einput-data\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\nInput: {user_input}\n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;/\u003c/span\u003einput-data\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;\u003c/span\u003eoutput\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\nSentiment: \n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;/\u003c/span\u003eoutput\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eJust imagine that the \u003ccode\u003e\u0026#x3C;tags\u003e\u003c/code\u003e are not actually present, and you've got the basic idea (although some models \u003cem\u003edo\u003c/em\u003e allow you to explicitly structure your prompts using XML! Anthropic's \u003ca href=\"https://docs.anthropic.com/claude/docs/use-xml-tags\"\u003eClaude\u003c/a\u003e models allow this, for example).\u003c/p\u003e\n\u003cp\u003eIf we replace \u003ccode\u003e{user_input}\u003c/code\u003e with a restaurant review like \u003ccode\u003eThe food was just OK\u003c/code\u003e, the model should respond with a classification. In this case, \u003ccode\u003eNeutral\u003c/code\u003e would probably be an acceptable response.\u003c/p\u003e\n\u003ch3 id=\"how-much-does-the-format-matter\"\u003eHow much does the format matter?\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#how-much-does-the-format-matter\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eShort answer: \u003cstrong\u003ea lot\u003c/strong\u003e. The size (number of parameters) and type of a model will have a significant impact on how the model responds to a prompt. The same prompt can get very different inputs from different models, and switching from one model to another will nearly always require adjusting your prompt.\u003c/p\u003e\n\u003cp\u003eMost LLMs also care about \u003cstrong\u003ethe order of information in the prompt itself\u003c/strong\u003e. \u003ca href=\"/coding-notes/(https://arxiv.org/abs/2307.03172).\"\u003eThis paper\u003c/a\u003e shows that \u003cstrong\u003eLLMs typically pay more attention to information from the \u003cem\u003ebeginning\u003c/em\u003e and \u003cem\u003eend\u003c/em\u003e of the prompt\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTip:\u003c/strong\u003e Always follow the model creator's guidelines for prompting. Most LLM providers have a set of best practices which should be followed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://docs.anthropic.com/claude/docs/prompt-engineering\"\u003eAnthropic Prompt Engineering Guide\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://platform.openai.com/docs/guides/prompt-engineering\"\u003eOpenAI Prompt Engineering Guide\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"prompt-engineering-methodology\"\u003ePrompt engineering methodology\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#prompt-engineering-methodology\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003ePrompt engineering can seem both overwhelming and very subjective. Are there any general principles we can use to improve prompt quality?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eYes\u003c/strong\u003e. One of the most popular frameworks at present is COSTAR:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eC\u003c/strong\u003eontext: Information about the task we are asking the LLM to do, and how it should be done.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eO\u003c/strong\u003ebjective: The purpose (goal) of the request. For instance, we might want to solve a coding issue, or craft a blog post.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eS\u003c/strong\u003etyle: The style of writing the LLM should use, for example \"technical\" or \"creative\".\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eT\u003c/strong\u003eone: The tone of the LLM's output, for instance \"casual\", \"friendly\", or \"professional\".\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eA\u003c/strong\u003eudience: Context about the people people the LLM is producing output for.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eR\u003c/strong\u003eesponse: The end goal the model should be striving for (for example \"accurate\" or \"relevant\" answers)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre class=\"language-xml\"\u003e\u003ccode class=\"language-xml\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;\u003c/span\u003econtext\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\nHelp create a Facebook post about the benefits of gardening. The focus should be on the health benefits of growing your own food and the psychological benefits of creating a gardening routine\n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;/\u003c/span\u003econtext\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;\u003c/span\u003eobjective\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\nCreate a post for me about growing tomato plants from seed in an indoor setting using instructions that even a beginner can follow.\n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;/\u003c/span\u003eobjective\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;\u003c/span\u003estyle\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\nUse a friendly, engaging writing style.\n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;/\u003c/span\u003estyle\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;\u003c/span\u003eaudience\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\nThe audience is people who are new to gardening but are interested in sustainable living, and health and wellness.\n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;/\u003c/span\u003eaudience\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;\u003c/span\u003eresponse\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\nThe format should be an accurate, engaging Facebook post.\n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;/\u003c/span\u003eresponse\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWithout the \u003ccode\u003e\u0026#x3C;tags\u003e\u003c/code\u003e removed, here is a response from Anthropic's Claude 3 Opus model:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e🌱🍅 Discover the Joys of Growing Tomatoes from Seed Indoors! 🌿🏡\n\nAre you curious about starting your own indoor tomato garden but not sure where to begin? Look no further! Growing tomatoes from seed is a rewarding and fun experience that anyone can enjoy, even if you're new to gardening. 🌱\n\nHere's a simple step-by-step guide to get you started:\n\n1. Choose your tomato seeds. There are many varieties available, so pick one that suits your taste and growing conditions. 🍅\n2. Fill small pots or seed trays with a well-draining seed starting mix.\n3. Plant the seeds about 1/4 inch deep and gently cover them with soil.\n4. Water the soil until it's moist but not soggy, and place the pots in a warm, sunny spot or under grow lights. ☀️\n5. Keep the soil moist and wait for the seeds to germinate, which usually takes 7-14 days.\n6. Once the seedlings have grown a few inches tall and have their first set of true leaves, transplant them into larger pots.\n7. Continue caring for your tomato plants by watering them regularly, providing plenty of light, and fertilizing them every few weeks. 💧\n\nNot only will you enjoy delicious, homegrown tomatoes, but you'll also reap the many health benefits of gardening! Studies show that gardening can reduce stress, improve mental health, and provide gentle physical activity. 💪😊\n\nPlus, by growing your own food, you'll know exactly what goes into it and can enjoy the freshest, most flavorful tomatoes possible. 🍽️\n\nSo why not give indoor tomato gardening a try? Share your experiences and photos in the comments below, and let's inspire each other to cultivate a healthier, more sustainable lifestyle! 🌍💚\n\n#IndoorGardening #TomatoesFromSeed #HealthyLiving #SustainableLiving #GardeningForBeginners\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that the model provided the emojis, engaging title, and hashtags all on its own. Compare that to the output below, which was generated from the barebones prompt \u003ccode\u003eCreate a facebook post about growing tomato plants indoors\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e🍅🌿 Attention all gardening enthusiasts! 🌱\n\nHave you ever wanted to enjoy fresh, juicy tomatoes year-round? Well, I've got great news for you! Growing tomatoes indoors is easier than you might think, and it's a fantastic way to have a constant supply of this delicious fruit. 🍅\n\nHere are a few tips to get you started:\n\n1. Choose the right variety: Opt for smaller, compact tomato plants that are well-suited for indoor growth, such as cherry or plum tomatoes. 🍒\n\n2. Provide plenty of light: Tomatoes need at least 6-8 hours of direct sunlight daily. If you don't have a sunny window, consider using grow lights to ensure your plants receive enough light. ☀️💡\n\n3. Maintain the right temperature: Tomatoes thrive in temperatures between 65-85°F (18-29°C). Keep them away from cold drafts and air conditioning vents. 🌡️\n\n4. Water and fertilize regularly: Keep the soil consistently moist but not soggy, and fertilize your plants every two weeks with a balanced, water-soluble fertilizer. 💦🌿\n\n5. Support your plants: As your tomato plants grow, provide them with a stake, cage, or trellis for support. This will keep the fruits off the ground and improve air circulation. 🪴\n\nGrowing tomatoes indoors is a rewarding experience that allows you to enjoy fresh, homegrown produce no matter the season! 🥗 If you have any tips or success stories about indoor tomato gardening, share them in the comments below! 👇\n\nHappy gardening, everyone! 🌿😊\n\n#IndoorGardening #TomatoPlants #GrowYourOwn #GardeningTips #FreshProduce\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eStill OK, but note that the prompt does not include specific, targeted information about physical and psychological benefits of gardening, and the instructions are not beginner friendly.\u003c/p\u003e\n\u003ch2 id=\"approaches-to-prompting\"\u003eApproaches to prompting\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#approaches-to-prompting\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eBeyond ensuring the prompt includes enough detail (by using a framework like CO-STAR), there are also some approaches to prompting that can give the LLM additional context about what we want it to do.\u003c/p\u003e\n\u003ch3 id=\"zero-shot-prompting\"\u003eZero-shot prompting\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#zero-shot-prompting\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA zero-shot prompt does not include any examples of the required response. The prompts we created above using CO-STAR were zero-shot prompts: we did not include any examples of the expected output.\u003c/p\u003e\n\u003cp\u003eZero-shot prompts are \u003cstrong\u003esimple to implement\u003c/strong\u003e and \u003cstrong\u003ecost-effective\u003c/strong\u003e. We use a single, relatively short prompt to produce a single output from the LLM. This reduces our token count, which means we pay less.\u003c/p\u003e\n\u003cp\u003eHowever zero-shot prompts can be \u003cem\u003ehard to get right\u003c/em\u003e and can produce \u003cstrong\u003eless precise\u003c/strong\u003e outputs.\u003c/p\u003e\n\u003cp\u003eOf course, one way to make an LLM better at zero-shot prompts is by fine-tuning it, as documented \u003ca href=\"https://arxiv.org/abs/2109.01652\"\u003ein this paper\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eCommercial models like GPT 4 also use RLHF (reinforcement learning with human feedback) to improve performance on zero-shot prompts, as documented \u003ca href=\"https://arxiv.org/abs/1706.03741\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"few-shot-prompting\"\u003eFew-shot prompting\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#few-shot-prompting\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eFew-shot prompts include one or more examples of the model's expected output.\u003c/p\u003e\n\u003cp\u003eMany LLM creators recommend always using few-shot prompting, because it helps guarantee appropriate output from the model.\u003c/p\u003e\n\u003cp\u003eHowever, \u003cstrong\u003emore work is needed\u003c/strong\u003e to craft the prompt (we must decide what examples to include), and \u003cstrong\u003ecosts will be higher\u003c/strong\u003e as the prompt is longer (and therefore contains more tokens, which will lead to higher costs when calling an LLM API that charges per-token).\u003c/p\u003e\n\u003cp\u003eKey papers, source from \u003ca href=\"https://www.promptingguide.ai/techniques/fewshot\"\u003ethis page\u003c/a\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2302.13971\"\u003eLLaMA: Open and Efficient Foundation Language Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2001.08361\"\u003eScaling Laws for Neural Language Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2005.14165\"\u003eLanguage Models are Few-Shot Learners\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2202.12837\"\u003eRethinking the Role of Demonstrations: What Makes In-Context Learning Work?\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"chain-of-thoughts-prompting-cot\"\u003eChain of Thoughts prompting (CoT)\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#chain-of-thoughts-prompting-cot\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThis type of prompting helps models to perform complex reasoning through intermediate reasoning steps.\u003c/p\u003e\n\u003cp\u003eThis improves performance on \"common sense\" reasoning tasks, as well as symbolic and mathematical reasoning.\u003c/p\u003e\n\u003cp\u003eSome models have specific features to enable this. \u003cstrong\u003eClaude\u003c/strong\u003e has a \u003ca href=\"https://docs.anthropic.com/claude/docs/let-claude-think\"\u003eset of xml tags\u003c/a\u003e, that allow you to see the model's reasoning when it responds to CoT prompts:\u003c/p\u003e\n\u003cpre class=\"language-xml\"\u003e\u003ccode class=\"language-xml\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;\u003c/span\u003ethinking\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003eThe model's thinking will appear here\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;/\u003c/span\u003ethinking\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;\u003c/span\u003eanswer\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003eThe model's answer to the end-user will appear here\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token tag\"\u003e\u003cspan class=\"token punctuation\"\u003e\u0026#x3C;/\u003c/span\u003eanswer\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e\u003e\u003c/span\u003e\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis can be useful when building a new CoT prompt. Related papers:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2201.11903\"\u003eChain-of-Thought Prompting Elicits Reasoning in Large Language Models\u003c/a\u003e, borrowed from \u003ca href=\"https://www.promptingguide.ai/techniques/cot\"\u003ethis page\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2205.11916\"\u003eLarge Language Models are Zero-Shot Reasoners\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"variation-zero-shot-cot\"\u003eVariation: Zero-shot CoT\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#variation-zero-shot-cot\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eA technique that sometimes works on larger models is simply prompting the model to think out its response. Phrases like \"let's think step by step\" can prompt some models to carry out a chain or reasoning on their own.\u003c/p\u003e\n\u003ch3 id=\"variation-auto-cot\"\u003eVariation: Auto-CoT\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#variation-auto-cot\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eAnother interesting approach proposed in the paper \u003ca href=\"https://arxiv.org/abs/2210.03493\"\u003eAutomatic Chain of Thought Prompting in Large Language Models\u003c/a\u003e, is to generate examples for few-shot CoT prompts automatically using \"think step by step\", then filtering or selecting reasoning examples from the resulting outputs. The code from the paper can be found \u003ca href=\"https://github.com/amazon-science/auto-cot\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"few-shot-with-intermediate-steps\"\u003eFew-shot with intermediate steps\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#few-shot-with-intermediate-steps\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThis approach lets you combine Chain-of-Thought with few-shot. The model can \"review\" information before answering.\u003c/p\u003e\n\u003cp\u003eThis can be very helpful when generating code (SQL, JSON). This can be a weak spot for many LLMs because the formatting requirements are so strict.\u003c/p\u003e\n\u003cp\u003eMore information on few-shot prompting can be found \u003ca href=\"https://www.promptingguide.ai/techniques/fewshot\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","noteIndex":{"id":"9qx3z8b2wfr5chnnb949zv2","title":"Coding Notes","desc":"Notes on writing, fixing, optimizing, and managing code","updated":1713239898794,"created":1710404034956,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":".","selfContained":true,"name":"coding-notes"},"contentHash":"343469aff8d1dd850480e3be33dce7c1","links":[],"anchors":{},"children":["z1cgzhp9dn5hy47dl69sgwr","zrzuso1wjne88yp1b9wtm8v","e1efo5mvaktzbf53sksb0k0","xk249rrm3kfwe5twpc1ouak","2oo9jjnro6kp9228ryfqdgw"],"parent":null,"data":{},"body":"\nWelcome! I keep all my coding notes here. Anything I learn about writing code, testing code, updating code, or optimizing code will go here.\n\nCheck back from time to time for updates. \n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":".","selfContained":true,"name":"coding-notes"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enablePersistentHistory":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal Knowledge Space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"search","siteUrl":"https://jeremypedersen.github.io","assetsPrefix":"/coding-notes","siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"qtq92f3bsp0mds91svqsxnx"},"buildId":"jdQmoR0T7zxn3zBS6JrtA","assetPrefix":"/coding-notes","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>